{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPQA Evaluation (Ollama)\n",
        "\n",
        "This notebook mirrors the **GPQA** closed-book evaluation flow based on the official GPQA repo, but implemented using the shared helpers in `llm_wc`.\n",
        "\n",
        "**Pipeline**\n",
        "1. Load GPQA questions (main or diamond) and shuffle answer choices deterministically.\n",
        "2. Build prompts (zero-shot, chain-of-thought, or 5-shot).\n",
        "3. Query the model and parse the multiple-choice answer.\n",
        "4. Run the evaluation loop and compute accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0 - Configuration\n",
        "Adjust these settings for your model, dataset path, and prompt type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bf20957e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd01 Autoreload is ON (IPython detected).\n",
            "\u2705 Using llm_wc from: /home/iamsikun/research/llm-wc/src/llm_wc\n"
          ]
        }
      ],
      "source": [
        "%run _dev_setup.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cd29b4b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "from typing import Any\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2b8ada8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llm_wc.gpqa import (\n",
        "    GPQA_CHOICES,\n",
        "    GPQA_DATASET,\n",
        "    GPQA_DEFAULT_COT_EXAMPLES,\n",
        "    GPQA_DEFAULT_PROMPT_TYPE,\n",
        "    GPQA_DEFAULT_SPLIT,\n",
        "    load_gpqa_benchmark,\n",
        "    prompt_gpqa,\n",
        "    evaluate_model_on_gpqa,\n",
        ")\n",
        "from llm_wc.client import ClientConfig, build_client\n",
        "from llm_wc.core import compute_accuracy\n",
        "from llm_wc.core.mcqa import extract_choice_answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1a38dfbe",
      "metadata": {},
      "outputs": [],
      "source": [
        "OLLAMA_URL = 'http://localhost:11434/v1'\n",
        "MODEL_NAME = 'gpt-oss:latest'\n",
        "\n",
        "# Ollama ignores the key, but the client expects it\n",
        "os.environ.setdefault('OPENAI_API_KEY', 'ollama')\n",
        "\n",
        "# GPQA config\n",
        "DATASET_PATH = GPQA_DATASET  # or local CSV directory\n",
        "SUBSET = 'main'  # 'diamond' for GPQA-Diamond\n",
        "SPLIT = GPQA_DEFAULT_SPLIT\n",
        "PROMPT_TYPE = GPQA_DEFAULT_PROMPT_TYPE  # zero_shot | chain_of_thought | 5_shot\n",
        "COT_EXAMPLES_PATH = GPQA_DEFAULT_COT_EXAMPLES\n",
        "\n",
        "SEED = 12345\n",
        "\n",
        "# Where to save raw predictions\n",
        "OUTPUT_DIR = Path('eval_results/ollama_gpqa')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1955c89b",
      "metadata": {},
      "source": [
        "### Gated dataset access\n",
        "GPQA is gated on Hugging Face. You can either authenticate via `huggingface-cli login` or export a token in an env var (recommended).\n",
        "\n",
        "- Set `HF_TOKEN_ENV` to the env var name that stores your token, or\n",
        "- Assign `HF_TOKEN` directly for an explicit override.\n",
        "\n",
        "These values are passed into `load_gpqa_examples()` and ultimately `datasets.load_dataset()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33e0e5f3",
      "metadata": {},
      "source": [
        "## Step 1 - Load the dataset\n",
        "The loader supports local CSVs (e.g., `gpqa_main.csv` / `gpqa_diamond.csv`) or the Hugging Face dataset name.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ea944c43",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "448"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "dataset = load_gpqa_benchmark(\n    dataset_path=DATASET_PATH,\n    subset=SUBSET,\n    split=SPLIT,\n    seed=SEED,\n    max_examples=LIMIT_QUESTIONS,\n    hf_token=HF_TOKEN,\n    hf_token_env=HF_TOKEN_ENV,\n)\nlen(dataset.questions)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question id: 0\n",
            "Question: A large gene has dozens of exons, of which the central ones code for folded triple helical repeats that connect the cytoskeleton with sarcolemma and extracellular space. Each exon usually codes for one folded triple alpha helix. The most common mutations of the gene are central exon deletions that create out-of-frame peptides and progressive degenerative organ waste. A solution is to deliver a Morpholino that recognizes the 5' end of the out-of-frame exon in pre-mRNA. The molecule prevents binding of the spliceosome and creates exon skipping and in-frame joining. Several missing exons are well tolerated by an organism. Which structure below is not involved in the proposed therapy?\n",
            "Choices:\n",
            "  0: polyA tail\n",
            "  1: lariat\n",
            "  2: antisense\n",
            "  3: R-loops\n",
            "Correct choice index: 3\n",
            "Correct letter: D\n"
          ]
        }
      ],
      "source": [
        "example = dataset.questions[0]\n",
        "print(f'Question id: {example.id}')\n",
        "print(f'Question: {example.question}')\n",
        "print('Choices:')\n",
        "for letter, choice in example.choices.items():\n",
        "    print(f'  {letter}: {choice}')\n",
        "print(f'Correct letter: {example.answer}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 - Build LLM client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "client_cfg = ClientConfig(\n",
        "    provider=\"openai\",\n",
        "    model=MODEL_NAME,\n",
        "    api_base=OLLAMA_URL,\n",
        "    api_key=\"ollama\",\n",
        ")\n",
        "client = build_client(client_cfg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 - Build the prompt\n",
        "Use `PROMPT_TYPE` to swap between the official GPQA prompt styles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'resources/gpqa/chain_of_thought_examples.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m prompt = \u001b[43mbuild_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mchain_of_thought\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcot_examples_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCOT_EXAMPLES_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(prompt)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/research/llm-wc/src/llm_wc/gpqa.py:390\u001b[39m, in \u001b[36mbuild_prompt\u001b[39m\u001b[34m(example, prompt_type, cot_examples_path)\u001b[39m\n\u001b[32m    387\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m prompt\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompt_type == \u001b[33m\"\u001b[39m\u001b[33mchain_of_thought\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     examples = \u001b[43m_load_cot_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcot_examples_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    391\u001b[39m     prompt = (\n\u001b[32m    392\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mHere are some example questions from experts. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    393\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn explanation is given before the final answer. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAnswer the final question yourself, giving your reasoning beforehand.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    395\u001b[39m     )\n\u001b[32m    396\u001b[39m     prompt += _render_cot_examples(examples, with_explanations=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/research/llm-wc/src/llm_wc/gpqa.py:342\u001b[39m, in \u001b[36m_load_cot_examples\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load chain-of-thought example data from a JSON file.\u001b[39;00m\n\u001b[32m    334\u001b[39m \n\u001b[32m    335\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    339\u001b[39m \u001b[33;03m    list[dict]: List of example question dictionaries.\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    341\u001b[39m resolved_path = _resolve_resource_path(path)\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mresolved_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m    343\u001b[39m     data = json.load(handle)\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data.get(\u001b[33m\"\u001b[39m\u001b[33mquestions\u001b[39m\u001b[33m\"\u001b[39m, [])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.11-linux-x86_64-gnu/lib/python3.13/pathlib/_local.py:537\u001b[39m, in \u001b[36mPath.open\u001b[39m\u001b[34m(self, mode, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m    536\u001b[39m     encoding = io.text_encoding(encoding)\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'resources/gpqa/chain_of_thought_examples.json'"
          ]
        }
      ],
      "source": "prompt = prompt_gpqa(\n    dataset,\n    example,\n    prompt_type=PROMPT_TYPE,\n    cot_examples_path=COT_EXAMPLES_PATH,\n)\nprint(prompt[:1200])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 - Query the model\n",
        "We request a model completion for a single GPQA prompt, then parse the answer letter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params: dict[str, Any] = {\n",
        "    'temperature': 0.0,\n",
        "    'max_tokens': 32768,\n",
        "    'top_p': 0.95,\n",
        "    'frequency_penalty': 0.0,\n",
        "    'presence_penalty': 0.0,\n",
        "    'logprobs': True,\n",
        "    'top_logprobs': 10,\n",
        "    'seed': 3,\n",
        "}\n",
        "\n",
        "message_text = [{\n",
        "    'role': 'user',\n",
        "    'content': prompt\n",
        "}]\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=MODEL_NAME,\n",
        "    messages=message_text,\n",
        "    **params,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "token_usage = completion.usage\n",
        "print(f'Total tokens: {token_usage.total_tokens}')\n",
        "print(f'  Prompt tokens: {token_usage.prompt_tokens}')\n",
        "print(f'  Completion tokens: {token_usage.completion_tokens}')\n",
        "\n",
        "message = completion.choices[0].message\n",
        "print('Model answer:')\n",
        "print(message.content)\n",
        "print('Model reasoning:')\n",
        "print(message.reasoning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parsed = extract_choice_answer(message.content, choices=GPQA_CHOICES)\n",
        "print(f'Parsed answer: {parsed.choice}')\n",
        "print(f'Matched pattern: {parsed.matched_pattern}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 - Run the evaluation loop\n",
        "Now run the full evaluation (optionally limited) and score accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "client_cfg = ClientConfig(\n    provider=\"openai\",\n    model=MODEL_NAME,\n    api_base=OLLAMA_URL,\n    api_key=\"ollama\",\n)\nclient = build_client(client_cfg)\n\nresults = evaluate_model_on_gpqa(\n    llm_client=client,\n    dataset_path=DATASET_PATH,\n    subset=SUBSET,\n    split=SPLIT,\n    seed=SEED,\n    max_examples=LIMIT_QUESTIONS,\n    question_ids=None,\n    prompt_type=PROMPT_TYPE,\n    cot_examples_path=COT_EXAMPLES_PATH,\n    hf_token=HF_TOKEN,\n    hf_token_env=HF_TOKEN_ENV,\n    request_params=params,\n    show_progress=False,\n)\nlen(results)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary = compute_accuracy(results)\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6 - Save results\n",
        "Persist raw predictions and the summary stats for later analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "(OUTPUT_DIR / 'results.json').write_text(json.dumps(results, indent=2))\n",
        "(OUTPUT_DIR / 'summary.json').write_text(json.dumps(summary, indent=2))\n",
        "OUTPUT_DIR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Notes\n",
        "- Use `SUBSET = 'diamond'` to evaluate the GPQA Diamond split.\n",
        "- `PROMPT_TYPE` supports `zero_shot`, `chain_of_thought`, and `5_shot`, matching the official GPQA closed-book baselines.\n",
        "- The choice order is shuffled with `SEED` to match the GPQA evaluation setup.\n",
        "- The dataset loader supports either Hugging Face (`idavidrein/gpqa`) or local CSVs.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}