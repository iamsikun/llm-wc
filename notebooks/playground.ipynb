{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing an Ollama Model on a Single SWE-Bench Task\n",
        "\n",
        "This notebook walks through **testing an Ollama model** on **one specific task** from SWE-Bench. It is written as a *procedure* with commands and checkpoints rather than a fully automated run, so you can adapt it to your environment.\n",
        "\n",
        "**What you will do**\n",
        "1. Pick a single SWE-Bench task ID.\n",
        "2. Prepare the repository and environment.\n",
        "3. Configure the Ollama model and runner.\n",
        "4. Run the task once and capture outputs.\n",
        "5. Evaluate the result with SWE-Bench‚Äôs evaluation logic.\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "- You have this repo cloned and can run the provided scripts.\n",
        "- You have Ollama installed and a model pulled (e.g., `ollama pull llama3.1`).\n",
        "- You can run Python in the project environment.\n",
        "\n",
        "> If you haven‚Äôt set up the environment, run the project‚Äôs standard setup procedure first (see repo README)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e5332940",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÅ Autoreload is ON (IPython detected).\n",
            "‚úÖ Using llm_wc from: /home/iamsikun/research/llm-wc/src/llm_wc\n"
          ]
        }
      ],
      "source": [
        "%run _dev_setup.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ed3bc15a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5d31f47e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">‚ï≠‚îÄ BREAKING CHANGES AHEAD ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">‚îÇ</span> <span style=\"font-weight: bold\">mini-swe-agent v2.0 is coming soon.</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">‚îÇ</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">‚îÇ</span>                                                                                                     <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">‚îÇ</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">‚îÇ</span> It will be even more flexible, extensible and performant. However, <span style=\"font-weight: bold\">breaking changes</span> were necessary. <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">‚îÇ</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">‚îÇ</span>                                                                                                     <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">‚îÇ</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">‚îÇ</span> To stay with the current major version for now, you can pin your dependency: <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">mini-swe-agent~=1.0</span>    <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">‚îÇ</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;31m‚ï≠‚îÄ\u001b[0m\u001b[1;31m BREAKING CHANGES AHEAD \u001b[0m\u001b[1;31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[1;31m‚îÄ‚ïÆ\u001b[0m\n",
              "\u001b[1;31m‚îÇ\u001b[0m \u001b[1mmini-swe-agent v2.0 is coming soon.\u001b[0m                                                                 \u001b[1;31m‚îÇ\u001b[0m\n",
              "\u001b[1;31m‚îÇ\u001b[0m                                                                                                     \u001b[1;31m‚îÇ\u001b[0m\n",
              "\u001b[1;31m‚îÇ\u001b[0m It will be even more flexible, extensible and performant. However, \u001b[1mbreaking changes\u001b[0m were necessary. \u001b[1;31m‚îÇ\u001b[0m\n",
              "\u001b[1;31m‚îÇ\u001b[0m                                                                                                     \u001b[1;31m‚îÇ\u001b[0m\n",
              "\u001b[1;31m‚îÇ\u001b[0m To stay with the current major version for now, you can pin your dependency: \u001b[1;37mmini-swe-agent~=1.0\u001b[0m    \u001b[1;31m‚îÇ\u001b[0m\n",
              "\u001b[1;31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üëã This is <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">mini-swe-agent</span> version <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.17</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span>.\n",
              "Loading global config from <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'/home/iamsikun/.config/mini-swe-agent/.env'</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "üëã This is \u001b[1;32mmini-swe-agent\u001b[0m version \u001b[1;32m1.17\u001b[0m\u001b[1;32m.\u001b[0m\u001b[1;32m4\u001b[0m.\n",
              "Loading global config from \u001b[1;32m'/home/iamsikun/.config/mini-swe-agent/.env'\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from minisweagent.run.extra import swebench as swebench_run\n",
        "from minisweagent.run.extra.utils.batch_progress import RunBatchProgressManager\n",
        "from minisweagent.config import get_config_path\n",
        "from swebench.harness import prepare_images as swebench_prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75ed54f9",
      "metadata": {},
      "source": [
        "## Step 1 ‚Äî Choose a single SWE-Bench task\n",
        "Pick **one** task ID from the SWE-Bench dataset.\n",
        "\n",
        "**Example task ID** (replace with any real task you want to test):\n",
        "- `swebench__requests-1929`\n",
        "\n",
        "**Checkpoint:** You should now have a single `TASK_ID` you want to run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ad5de689",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51251294a40f4c51a0617110bf083e1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fc123d0565e45b0a761be72a3e9fa0e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/test-00000-of-00001.parquet:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5cf21ab365af4fb0b8073bcfd1f16cc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset_name = \"princeton-nlp/SWE-Bench_Verified\"\n",
        "split = \"test\"\n",
        "\n",
        "ds = load_dataset(dataset_name, split=split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0da328bc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: princeton-nlp/SWE-Bench_Verified\n",
            "Number of task IDs: 500\n",
            "First 5 task IDs: ['astropy__astropy-12907', 'astropy__astropy-13033', 'astropy__astropy-13236', 'astropy__astropy-13398', 'astropy__astropy-13453']\n"
          ]
        }
      ],
      "source": [
        "task_ids: list = [row[\"instance_id\"] for row in ds]\n",
        "\n",
        "print(f\"Dataset: {dataset_name}\")\n",
        "print(f\"Number of task IDs: {len(task_ids)}\")\n",
        "print(f\"First 5 task IDs: {task_ids[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb02a6d5",
      "metadata": {},
      "source": [
        "## Step 2 ‚Äî Configure the run (pure Python)\n",
        "This replaces the shell script in `scripts/run_swebench_verified.sh` with direct Python calls.\n",
        "We will set up the task ID, model, config path, and output directory.\n",
        "\n",
        "**Checkpoint:** You have `TASK_ID`, `MODEL_NAME`, `CONFIG_PATH`, and `OUTPUT_DIR` defined.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0128ab92",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TASK_ID: astropy__astropy-12907\n",
            "MODEL_NAME: ollama/gemma3:4b\n",
            "CONFIG_PATH: ../config/swebench_ollama.yaml\n",
            "OUTPUT_DIR: runs/swebench_verified/ollama_gemma3_4b/single_task\n",
            "Dataset: princeton-nlp/SWE-Bench_Verified\n",
            "Split: test\n"
          ]
        }
      ],
      "source": [
        "# Pick a task ID (override this if you want a specific instance)\n",
        "TASK_ID = task_ids[0]\n",
        "\n",
        "# Ollama model to test (must be available in Ollama)\n",
        "MODEL_NAME = \"ollama/gemma3:4b\"\n",
        "\n",
        "# Config used by mini-swe-agent (Ollama defaults in this repo)\n",
        "CONFIG_PATH = Path(\"../config/swebench_ollama.yaml\")\n",
        "\n",
        "# Dataset to use\n",
        "DATASET_NAME = dataset_name\n",
        "SPLIT = split\n",
        "\n",
        "# Match the output directory layout used by scripts/run_swebench_verified.sh\n",
        "sanitized_model = MODEL_NAME.replace(\"/\", \"_\").replace(\":\", \"_\")\n",
        "OUTPUT_DIR = Path(\"runs/swebench_verified\") / sanitized_model / \"single_task\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"TASK_ID: {TASK_ID}\")\n",
        "print(f\"MODEL_NAME: {MODEL_NAME}\")\n",
        "print(f\"CONFIG_PATH: {CONFIG_PATH}\")\n",
        "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")\n",
        "print(f\"Dataset: {DATASET_NAME}\")\n",
        "print(f\"Split: {SPLIT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d61b379c",
      "metadata": {},
      "source": [
        "## Step 3 ‚Äî Run a single SWE-Bench instance (pure Python)\n",
        "This calls the **same mini-swe-agent logic** as `mini-extra swebench`, but runs a single instance directly.\n",
        "\n",
        "**Mac/ARM note:** SWE-Bench images on DockerHub are built for x86_64. On Apple Silicon,\n",
        "build the instance image locally and override `image_name` so the runner uses your local image.\n",
        "\n",
        "**Checkpoint:** `preds.json` appears in `OUTPUT_DIR`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e411bca1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All images exist. Nothing left to build.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">minisweagent.environment: DEBUG: Starting container with command: docker run -d --name minisweagent-2c5d28c5 -w    \n",
              "<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">testbed</span> --rm sweb.eval.x86_64.astropy__astropy-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12907</span>:latest sleep 2h                                              \n",
              "</pre>\n"
            ],
            "text/plain": [
              "minisweagent.environment: DEBUG: Starting container with command: docker run -d --name minisweagent-2c5d28c5 -w    \n",
              "\u001b[35m/\u001b[0m\u001b[95mtestbed\u001b[0m --rm sweb.eval.x86_64.astropy__astropy-\u001b[1;36m12907\u001b[0m:latest sleep 2h                                              \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-23 21:21:45,750 - minisweagent.environment - DEBUG - Starting container with command: docker run -d --name minisweagent-2c5d28c5 -w /testbed --rm sweb.eval.x86_64.astropy__astropy-12907:latest sleep 2h\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">minisweagent.environment: INFO: Started container minisweagent-2c5d28c5 with ID                                    \n",
              "e1a648ebc3ea334d76ec9deab54ff500d5fc245d4807e7990c3779dca0e8da70                                                   \n",
              "</pre>\n"
            ],
            "text/plain": [
              "minisweagent.environment: INFO: Started container minisweagent-2c5d28c5 with ID                                    \n",
              "e1a648ebc3ea334d76ec9deab54ff500d5fc245d4807e7990c3779dca0e8da70                                                   \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-23 21:21:46,137 - minisweagent.environment - INFO - Started container minisweagent-2c5d28c5 with ID e1a648ebc3ea334d76ec9deab54ff500d5fc245d4807e7990c3779dca0e8da70\n",
            "\u001b[92m21:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:21:46,160 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:21:57 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:21:57,209 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:21:58 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:21:58,430 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:22:06 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:22:06,258 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:22:07 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:22:07,274 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:22:17 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:22:17,879 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:22:18 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:22:18,887 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:22:29 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:22:29,842 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:22:30 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:22:30,998 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:22:41 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:22:41,469 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:22:42 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:22:42,428 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:23:08 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:23:08,978 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:23:10 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:23:10,053 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:23:20 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:23:20,730 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:23:21 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:23:21,194 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:23:30 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:23:30,679 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:23:31 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:23:31,636 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:23:46 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:23:46,123 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:23:46 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:23:46,754 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:23:56 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:23:56,106 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:23:56 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:23:56,565 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:24:05 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:24:05,814 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:24:06 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:24:06,845 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:24:16 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:24:16,555 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:24:17 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:24:17,062 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:24:26 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:24:26,992 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:24:27 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:24:27,625 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:24:37 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:24:37,705 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:24:38 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:24:38,168 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:24:47 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:24:47,831 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:24:48 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:24:48,290 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:24:57 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:24:57,995 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:24:58 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:24:58,449 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:25:06,956 - litellm_model - WARNING - Retrying minisweagent.models.litellm_model.LitellmModel._query in 4.0 seconds as it raised APIConnectionError: litellm.APIConnectionError: OllamaException - {\"error\":\"model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details\"}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m21:25:10 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:25:10,967 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:25:22,365 - litellm_model - WARNING - Retrying minisweagent.models.litellm_model.LitellmModel._query in 4.0 seconds as it raised APIConnectionError: litellm.APIConnectionError: OllamaException - {\"error\":\"model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details\"}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m21:25:26 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:25:26,379 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:25:39 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:25:39,417 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:25:39 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:25:39,912 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:25:49 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:25:49,730 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:25:50 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:25:50,317 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:26:00 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:26:00,099 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:26:00 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:26:00,586 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:26:10 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:26:10,260 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:26:10 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:26:10,728 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:26:20 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:26:20,342 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:26:20 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:26:20,784 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:26:30 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:26:30,778 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:26:31 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:26:31,441 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:26:40,095 - litellm_model - WARNING - Retrying minisweagent.models.litellm_model.LitellmModel._query in 4.0 seconds as it raised APIConnectionError: litellm.APIConnectionError: OllamaException - {\"error\":\"model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details\"}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m21:26:44 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:26:44,115 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:26:57 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:26:57,280 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:26:57 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:26:57,768 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:27:07 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:27:07,379 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:27:07 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:27:07,839 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:27:17 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:27:17,498 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:27:17 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:27:17,940 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:27:27 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:27:27,582 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:27:28 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:27:28,037 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:27:37 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:27:37,750 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:27:38 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:27:38,341 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:27:48 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:27:48,140 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:27:48 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:27:48,604 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:27:58 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:27:58,191 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:27:58 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:27:58,915 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:28:09 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:28:09,224 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:28:09 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:28:09,659 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:28:19 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:28:19,289 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:28:19 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:28:19,730 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:28:29 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:28:29,349 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:28:29 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:28:29,804 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:28:39 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:28:39,448 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:28:39 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:28:39,894 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:28:49 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:28:49,560 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:28:50 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:28:50,036 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:28:58 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:28:58,601 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:28:59 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:28:59,112 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:29:08 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:29:08,888 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:29:09 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:29:09,492 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:29:18 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:29:18,458 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:29:20 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:29:20,063 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:29:29 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:29:29,811 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:29:30 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:29:30,279 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:29:39 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:29:39,929 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:29:40 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:29:40,386 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:29:48 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:29:48,915 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:29:49 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:29:49,356 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:29:57,816 - litellm_model - WARNING - Retrying minisweagent.models.litellm_model.LitellmModel._query in 4.0 seconds as it raised APIConnectionError: litellm.APIConnectionError: OllamaException - {\"error\":\"model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details\"}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m21:30:01 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:30:01,828 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:30:15 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:30:15,059 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:30:15 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:30:15,543 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:30:25 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:30:25,352 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:30:25 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:30:25,797 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:30:35 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:30:35,577 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:30:36 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:30:36,056 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:30:44,486 - litellm_model - WARNING - Retrying minisweagent.models.litellm_model.LitellmModel._query in 4.0 seconds as it raised APIConnectionError: litellm.APIConnectionError: OllamaException - {\"error\":\"model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details\"}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m21:30:48 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:30:48,492 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:31:00,068 - litellm_model - WARNING - Retrying minisweagent.models.litellm_model.LitellmModel._query in 4.0 seconds as it raised APIConnectionError: litellm.APIConnectionError: OllamaException - {\"error\":\"model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details\"}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m21:31:04 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:31:04,080 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:31:16 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:31:16,705 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:31:17 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:31:17,173 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:31:26 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:31:26,957 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:31:27 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:31:27,404 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:31:37 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:31:37,278 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:31:37 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:31:37,728 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:31:47 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:31:47,587 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:31:48 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:31:48,046 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:31:57 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:31:57,875 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:31:58 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:31:58,341 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:32:08 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:32:08,115 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:32:08 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:32:08,534 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:32:17 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:32:17,035 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:32:17 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:32:17,458 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:32:27 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:32:27,219 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:32:27 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:32:27,706 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:32:37 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:32:37,457 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:32:37 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:32:37,879 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:32:47 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:32:47,611 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:32:48 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:32:48,028 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:32:57 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:32:57,808 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:32:58 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:32:58,283 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:33:07 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:33:07,045 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:33:07 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:33:07,492 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:33:17 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:33:17,249 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:33:17 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:33:17,719 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:33:26 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:33:26,268 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:33:26 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:33:26,705 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:33:35 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:33:35,386 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:33:35 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:33:35,821 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:33:44 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:33:44,336 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:33:44 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:33:44,752 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:33:54 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:33:54,536 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:33:54 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:33:54,970 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:34:03 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:34:03,540 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:34:03 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:34:03,950 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:34:13 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:34:13,695 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:34:14 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:34:14,101 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:34:22 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:34:22,744 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:34:23 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:34:23,239 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:34:33 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:34:33,051 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:34:33 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:34:33,643 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:34:44 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:34:44,332 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:34:44 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:34:44,801 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:34:54 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:34:54,571 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:34:55 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:34:55,037 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:35:03 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:35:03,725 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:35:04 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:35:04,186 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:35:12,625 - litellm_model - WARNING - Retrying minisweagent.models.litellm_model.LitellmModel._query in 4.0 seconds as it raised APIConnectionError: litellm.APIConnectionError: OllamaException - {\"error\":\"model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details\"}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m21:35:16 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:35:16,633 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:35:27,933 - litellm_model - WARNING - Retrying minisweagent.models.litellm_model.LitellmModel._query in 4.0 seconds as it raised APIConnectionError: litellm.APIConnectionError: OllamaException - {\"error\":\"model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details\"}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m21:35:31 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:35:31,946 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:35:44 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:35:44,631 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:35:45 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:35:45,073 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:35:54 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:35:54,816 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:35:55 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:35:55,231 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:36:04 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:36:04,990 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:36:05 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:36:05,417 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:36:14 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:36:14,064 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:36:14 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:36:14,470 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:36:23 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:36:23,166 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:36:23 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:36:23,618 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:36:32 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:36:32,155 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:36:32 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:36:32,570 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:36:42 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:36:42,438 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:36:42,881 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:36:52 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:36:52,634 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:36:53 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:36:53,092 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:37:01,563 - litellm_model - WARNING - Retrying minisweagent.models.litellm_model.LitellmModel._query in 4.0 seconds as it raised APIConnectionError: litellm.APIConnectionError: OllamaException - {\"error\":\"model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details\"}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m21:37:05 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:37:05,574 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:37:18 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:37:18,227 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:37:18 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:37:18,702 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:37:27 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:37:27,363 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:37:27 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:37:27,774 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:37:36 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:37:36,433 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:37:36 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:37:36,862 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:37:45 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:37:45,517 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:37:45 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:37:45,925 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:37:54 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:37:54,615 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:37:55 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:37:55,069 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:38:03 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:38:03,631 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:38:04 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:38:04,051 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:38:13 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:38:13,853 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:38:14 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:38:14,377 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:38:24 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:38:24,200 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:38:24 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:38:24,656 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:38:33 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:38:33,208 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:38:33 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:38:33,638 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:38:43 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:38:43,425 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:38:43 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:38:43,842 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:38:53 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:38:53,590 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:38:54 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:38:54,024 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:39:02 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:39:02,755 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:39:03 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:39:03,210 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:39:11 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:39:11,891 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:39:12 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:39:12,301 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:39:20 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:39:20,858 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:39:21 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:39:21,303 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:39:31 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:39:31,083 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:39:31 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:39:31,543 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:39:40 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:39:40,238 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:39:40 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:39:40,686 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:39:49 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:39:49,383 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:39:49 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:39:49,878 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:39:58 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:39:58,629 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:39:59 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:39:59,095 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:40:07 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:40:07,759 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:40:08 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:40:08,183 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:40:16 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:40:16,871 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:40:17 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:40:17,280 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:40:25,785 - litellm_model - WARNING - Retrying minisweagent.models.litellm_model.LitellmModel._query in 4.0 seconds as it raised APIConnectionError: litellm.APIConnectionError: OllamaException - {\"error\":\"model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details\"}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m21:40:29 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:40:29,798 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:40:42 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:40:42,608 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:40:43 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:40:43,057 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:40:51 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:40:51,645 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:40:52 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:40:52,211 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:41:03 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:41:03,847 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:41:04 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:41:04,354 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:41:14 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:41:14,181 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:41:14 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:41:14,643 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:41:24 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:41:24,591 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:41:25 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:41:25,221 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:41:34 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:41:34,003 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:41:34 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:41:34,424 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:41:43 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:41:43,096 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:41:43 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:41:43,515 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:41:53 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:41:53,321 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:41:53 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:41:53,792 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:42:02 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:42:02,520 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:42:02 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:42:02,983 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:42:11 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:42:11,682 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:42:12 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:42:12,105 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:42:21 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:42:21,893 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:42:22 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:42:22,303 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:42:32 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:42:32,111 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:42:32 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:42:32,535 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:42:41 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:42:41,118 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:42:41 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:42:41,546 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:42:51 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:42:51,370 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:42:51 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:42:51,816 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:43:00 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:43:00,559 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:43:01 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:43:01,007 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:43:09 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:43:09,757 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:43:10 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:43:10,200 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:43:18,637 - litellm_model - WARNING - Retrying minisweagent.models.litellm_model.LitellmModel._query in 4.0 seconds as it raised APIConnectionError: litellm.APIConnectionError: OllamaException - {\"error\":\"model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details\"}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m21:43:22 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:43:22,642 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:43:34 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:43:34,422 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:43:34 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:43:34,902 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:43:43 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:43:43,520 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:43:43 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:43:43,942 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:43:52 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:43:52,739 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:43:53 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:43:53,383 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:44:03 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:44:03,357 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:44:03 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:44:03,813 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:44:13 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:44:13,736 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:44:14 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:44:14,304 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:44:23 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:44:23,211 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:44:23 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:44:23,655 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:44:32 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:44:32,569 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:44:33 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:44:33,026 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:44:41 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:44:41,707 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:44:42 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:44:42,124 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:44:51 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:44:51,902 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:44:52 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:44:52,332 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:45:02 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:45:02,199 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:45:02 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:45:02,631 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:45:11 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:45:11,271 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:45:11 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:45:11,707 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:45:21 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:45:21,692 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:45:22 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:45:22,136 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:45:30 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:45:30,873 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:45:31 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:45:31,305 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:45:40 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:45:40,064 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:45:40 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:45:40,504 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:45:49 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:45:49,266 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:45:49 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:45:49,680 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:45:59 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:45:59,489 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:45:59 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:45:59,938 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:46:08 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:46:08,748 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:46:09 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:46:09,195 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:46:17,699 - litellm_model - WARNING - Retrying minisweagent.models.litellm_model.LitellmModel._query in 4.0 seconds as it raised APIConnectionError: litellm.APIConnectionError: OllamaException - {\"error\":\"model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details\"}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m21:46:21 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:46:21,710 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:46:33 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:46:33,564 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:46:34 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:46:34,051 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:46:44 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:46:44,052 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:46:44 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:46:44,506 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:46:54 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:46:54,367 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:46:54 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:46:54,906 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:47:03 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:47:03,804 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:47:04 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:47:04,275 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:47:14 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:47:14,154 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:47:14 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:47:14,618 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:47:23 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:47:23,365 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:47:23 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:47:23,796 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:47:32 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:47:32,548 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:47:33 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:47:33,012 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:47:43 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:47:43,013 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:47:43 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:47:43,469 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:47:53 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:47:53,327 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:47:53 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:47:53,794 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:48:02 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:48:02,486 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:48:02 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:48:02,926 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:48:13 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:48:13,024 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:48:13 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:48:13,600 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:48:22 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:48:22,740 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:48:23 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:48:23,199 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:48:32 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:48:32,343 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:48:33 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:48:33,211 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:48:43 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:48:43,460 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:48:43 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:48:43,915 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:48:52 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:48:52,699 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:48:53 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:48:53,184 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:49:01 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:49:01,959 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:49:02 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:49:02,435 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:49:12 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:49:12,523 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:49:13 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:49:13,003 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:49:21 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:49:21,703 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:49:22 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:49:22,177 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:49:32 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:49:32,898 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:49:33 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:49:33,348 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:49:42 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:49:42,028 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:49:42 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:49:42,486 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:49:51 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:49:51,150 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:49:51 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:49:51,614 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:50:01 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:50:01,724 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:50:02 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:50:02,349 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:50:11 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:50:11,257 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:50:11 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:50:11,693 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:50:20 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:50:20,470 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:50:20 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:50:20,934 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:50:30 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:50:30,817 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:50:31 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:50:31,269 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:50:41 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:50:41,218 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:50:41 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:50:41,692 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:50:50 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:50:50,754 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:50:51 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:50:51,206 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:50:59 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:50:59,985 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:51:00 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:51:00,445 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:51:10 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:51:10,523 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:51:10 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:51:10,973 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:51:20 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:51:20,149 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:51:20 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:51:20,919 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:51:30 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:51:30,663 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:51:31 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:51:31,144 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:51:41 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:51:41,087 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:51:41 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:51:41,546 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:51:51 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:51:51,536 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:51:51 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:51:51,978 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:52:00 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:52:00,801 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:52:01 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:52:01,270 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:52:11 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:52:11,324 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:52:11 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:52:11,778 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:52:20 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:52:20,578 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:52:21 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:52:21,107 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:52:29 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:52:29,790 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:52:30 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:52:30,236 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:52:40 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:52:40,173 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:52:40 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:52:40,622 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:52:50 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:52:50,494 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:52:50 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:52:50,948 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:52:59 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:52:59,718 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:53:00 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:53:00,382 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:53:10 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:53:10,609 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:53:11 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:53:11,096 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:53:19 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:53:19,826 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:53:20 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:53:20,271 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:53:29 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:53:29,034 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:53:29 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:53:29,479 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:53:39 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:53:39,432 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:53:39 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:53:39,888 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:53:48 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:53:48,667 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:53:49 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:53:49,112 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:53:58 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:53:58,003 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:53:58 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:53:58,474 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:54:08 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:54:08,363 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:54:08 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:54:08,813 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:54:18 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:54:18,731 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:54:19 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:54:19,177 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:54:27,721 - litellm_model - WARNING - Retrying minisweagent.models.litellm_model.LitellmModel._query in 4.0 seconds as it raised APIConnectionError: litellm.APIConnectionError: OllamaException - {\"error\":\"model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details\"}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m21:54:31 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:54:31,738 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:54:43 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:54:43,927 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:54:44 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:54:44,437 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:54:54 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:54:54,329 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:54:54 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:54:54,759 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:55:03 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:55:03,561 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:55:04 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:55:04,009 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:55:12 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:55:12,890 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:55:13 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:55:13,372 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:55:22 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:55:22,107 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:55:22 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:55:22,683 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:55:31 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:55:31,537 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:55:31 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:55:31,979 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:55:40 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:55:40,961 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:55:41 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:55:41,412 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:55:50 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:55:50,257 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:55:50 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:55:50,713 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:55:59 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:55:59,588 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:56:00 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:56:00,094 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:56:09 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:56:09,363 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:56:09 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:56:09,824 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:56:18 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:56:18,626 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:56:19 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:56:19,071 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:56:27 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:56:27,949 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:56:28 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:56:28,392 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:56:37 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:56:37,190 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:56:37 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:56:37,628 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:56:46 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:56:46,521 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:56:47 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:56:47,004 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:56:56 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:56:56,250 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:56:56 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:56:56,686 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:57:05,468 - litellm_model - WARNING - Retrying minisweagent.models.litellm_model.LitellmModel._query in 4.0 seconds as it raised APIConnectionError: litellm.APIConnectionError: OllamaException - {\"error\":\"model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details\"}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m21:57:09 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:57:09,489 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:57:23 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:57:23,257 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:57:23 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:57:23,695 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:57:32 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:57:32,702 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:57:33 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:57:33,161 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:57:42 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:57:42,127 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:57:42 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:57:42,584 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:57:51 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:57:51,343 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:57:51 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:57:51,773 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:58:00 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:58:00,685 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:58:01 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:58:01,124 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:58:10 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:58:10,006 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:58:10 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:58:10,499 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:58:19 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:58:19,544 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:58:20 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:58:20,017 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:58:29 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:58:29,105 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:58:29 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:58:29,769 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:58:40 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:58:40,905 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:58:41 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:58:41,396 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:58:50 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:58:50,462 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:58:51 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:58:51,001 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:59:01 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:59:01,936 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:59:02 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:59:02,444 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:59:11 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:59:11,557 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:59:12 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:59:12,089 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:59:21 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:59:21,229 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:59:21 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:59:21,706 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:59:30 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:59:30,760 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:59:31 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:59:31,188 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:59:40 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:59:40,260 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:59:40 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:59:40,764 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m21:59:51 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 21:59:51,433 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m21:59:52 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 21:59:52,272 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:00:01 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:00:01,531 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:00:01 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:00:01,968 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:00:10 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:00:10,918 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:00:11 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:00:11,355 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:00:20 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:00:20,174 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:00:20 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:00:20,617 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:00:29 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:00:29,420 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:00:29 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:00:29,851 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:00:38 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:00:38,678 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:00:39 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:00:39,115 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:00:47 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:00:47,958 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:00:48 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:00:48,424 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:00:57 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:00:57,413 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:00:57 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:00:57,848 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:01:06 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:01:06,772 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:01:07 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:01:07,246 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:01:16 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:01:16,193 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:01:16 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:01:16,652 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:01:25 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:01:25,544 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:01:25 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:01:25,986 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:01:35 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:01:35,016 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:01:35 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:01:35,629 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:01:44 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:01:44,894 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:01:45 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:01:45,361 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:01:54 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:01:54,624 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:01:55 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:01:55,414 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:02:06 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:02:06,019 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:02:06 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:02:06,496 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:02:15 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:02:15,580 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:02:16 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:02:16,081 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:02:25 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:02:25,887 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:02:26 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:02:26,398 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:02:35 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:02:35,385 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:02:35 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:02:35,862 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:02:44 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:02:44,934 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:02:45 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:02:45,381 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:02:54 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:02:54,239 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:02:54 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:02:54,689 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:03:03 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:03:03,722 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:03:04 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:03:04,343 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:03:13 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:03:13,853 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:03:14 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:03:14,298 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:03:23 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:03:23,359 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:03:23 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:03:23,868 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:03:33 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:03:33,010 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:03:33 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:03:33,465 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:03:42 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:03:42,476 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:03:42 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:03:42,935 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:03:52 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:03:52,014 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:03:52 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:03:52,574 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:04:02 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:04:02,193 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:04:02 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:04:02,754 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:04:13 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:04:13,223 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:04:13 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:04:13,866 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:04:27 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:04:27,977 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:04:29 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:04:29,829 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:04:43 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:04:43,306 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:04:43 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:04:43,712 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:04:52 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:04:52,636 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:04:53 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:04:53,031 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:05:01 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:05:01,875 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:05:02 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:05:02,287 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:05:11 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:05:11,277 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:05:11 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:05:11,748 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:05:21 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:05:21,304 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:05:21 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:05:21,797 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:05:30 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:05:30,850 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:05:31 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:05:31,368 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:05:40 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:05:40,849 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:05:41 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:05:41,338 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:05:51 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:05:51,559 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:05:52 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:05:52,392 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:06:04 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:06:04,618 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:06:05 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:06:05,199 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:06:16 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:06:16,381 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:06:16 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:06:16,902 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:06:26 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:06:26,383 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:06:27 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:06:27,077 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:06:36 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:06:36,554 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:06:37 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:06:37,014 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:06:46 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:06:46,327 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:06:46 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:06:46,781 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:06:56 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:06:56,159 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m22:06:56 - LiteLLM:INFO\u001b[0m: utils.py:3871 - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "2026-01-23 22:06:56,609 - LiteLLM - INFO - \n",
            "LiteLLM completion() model= gemma3:4b; provider = ollama\n",
            "\u001b[92m22:07:05 - LiteLLM:INFO\u001b[0m: utils.py:1620 - Wrapper: Completed Call, calling success_handler\n",
            "2026-01-23 22:07:05,799 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">minisweagent: INFO: Saved trajectory to                                                                            \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'runs/swebench_verified/ollama_gemma3_4b/single_task/astropy__astropy-12907/astropy__astropy-12907.traj.json'</span>      \n",
              "</pre>\n"
            ],
            "text/plain": [
              "minisweagent: INFO: Saved trajectory to                                                                            \n",
              "\u001b[32m'runs/swebench_verified/ollama_gemma3_4b/single_task/astropy__astropy-12907/astropy__astropy-12907.traj.json'\u001b[0m      \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-23 22:07:06,299 - minisweagent - INFO - Saved trajectory to 'runs/swebench_verified/ollama_gemma3_4b/single_task/astropy__astropy-12907/astropy__astropy-12907.traj.json'\n"
          ]
        }
      ],
      "source": [
        "# Load the instance by ID\n",
        "instance = next(row for row in ds if row['instance_id'] == TASK_ID)\n",
        "\n",
        "# For Apple Silicon/ARM: build the image locally and override image_name\n",
        "USE_LOCAL_IMAGES = True\n",
        "if USE_LOCAL_IMAGES:\n",
        "    local_image = f\"sweb.eval.x86_64.{TASK_ID.lower()}:latest\"\n",
        "    swebench_prepare.main(\n",
        "        dataset_name=DATASET_NAME,\n",
        "        split=SPLIT,\n",
        "        instance_ids=[TASK_ID],\n",
        "        max_workers=1,\n",
        "        force_rebuild=False,\n",
        "        open_file_limit=8192,\n",
        "        namespace=None,\n",
        "        tag=\"latest\",\n",
        "        env_image_tag=\"latest\",\n",
        "    )\n",
        "    instance[\"image_name\"] = local_image\n",
        "\n",
        "# Load and override config (same as CLI logic)\n",
        "config_path = get_config_path(CONFIG_PATH)\n",
        "config = yaml.safe_load(config_path.read_text())\n",
        "config.setdefault(\"model\", {})[\"model_name\"] = MODEL_NAME\n",
        "\n",
        "progress = RunBatchProgressManager(1, OUTPUT_DIR / \"exit_statuses.yaml\")\n",
        "swebench_run.process_instance(instance, OUTPUT_DIR, config, progress)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1a5065a",
      "metadata": {},
      "source": [
        "## Step 4 ‚Äî Inspect the output (pure Python)\n",
        "The batch runner writes a `preds.json` file with the model patch for the instance.\n",
        "\n",
        "**Checkpoint:** You can see the patch for `TASK_ID`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e48465c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "preds_path = OUTPUT_DIR / \"preds.json\"\n",
        "print(f\"preds.json exists: {preds_path.exists()}\")\n",
        "\n",
        "if preds_path.exists():\n",
        "    preds = json.loads(preds_path.read_text())\n",
        "    entry = preds.get(TASK_ID)\n",
        "    print(f\"Keys in entry: {list(entry.keys()) if entry else None}\")\n",
        "    if entry:\n",
        "        print(\"\\n--- Patch preview ---\\n\")\n",
        "        print(entry.get(\"model_patch\", \"\")[:1000])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be3c286b",
      "metadata": {},
      "source": [
        "## Step 5 ‚Äî Evaluate the result (pure Python)\n",
        "This calls the SWE-bench evaluation harness directly (same as `python -m swebench.harness.run_evaluation`).\n",
        "\n",
        "**Mac/ARM note:** pass `namespace=None` to build images locally instead of pulling x86_64 images.\n",
        "\n",
        "**Checkpoint:** You get a PASS/FAIL result for the single instance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from swebench.harness import run_evaluation as swebench_eval\n",
        "\n",
        "preds_path = OUTPUT_DIR / \"preds.json\"\n",
        "run_id = f\"{MODEL_NAME.replace('/', '__')}_single\"\n",
        "\n",
        "swebench_eval.main(\n",
        "    dataset_name=\"princeton-nlp/SWE-Bench_Verified\",\n",
        "    split=\"test\",\n",
        "    instance_ids=[TASK_ID],\n",
        "    predictions_path=str(preds_path),\n",
        "    max_workers=1,\n",
        "    force_rebuild=False,\n",
        "    cache_level=\"all\",\n",
        "    clean=False,\n",
        "    open_file_limit=4096,\n",
        "    run_id=run_id,\n",
        "    timeout=900,\n",
        "    namespace=None,\n",
        "    rewrite_reports=False,\n",
        "    modal=False,\n",
        "    instance_image_tag=\"latest\",\n",
        "    env_image_tag=\"latest\",\n",
        "    report_dir=\".\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6 ‚Äî Record the experiment\n",
        "Capture a short summary so you can compare runs later.\n",
        "\n",
        "Suggested fields:\n",
        "- **Task ID**: `TASK_ID`\n",
        "- **Model**: `MODEL_NAME`\n",
        "- **Config**: `CONFIG_PATH`\n",
        "- **Output**: `OUTPUT_DIR`\n",
        "- **Result**: PASS/FAIL\n",
        "- **Notes**: Any errors, retries, or unusual behavior\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Troubleshooting tips\n",
        "- **Model not found**: confirm the model exists in Ollama and that the name matches `MODEL_NAME`.\n",
        "- **Missing `datasets`**: install the `datasets` package in the notebook kernel environment.\n",
        "- **Docker errors**: SWE-bench runs inside containers; ensure Docker is running.\n",
        "- **Evaluation timeouts**: increase `timeout` in the evaluation cell if needed.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
